{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLJkaXucb4iuj6U/xK555K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagxf/interpret/blob/main/saliency-maps/loading_transformers_allennlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a HUggingFace model into AllenNLP"
      ],
      "metadata": {
        "id": "Lf_HquFtLK_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers allennlp eli5 --quiet\n",
        "%pip install -U google-cloud-storage==1.40.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddSalTymLIHQ",
        "outputId": "8e0d7b65-6ca5-406e-8521-96c1de42d870"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 216 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 56.6 MB/s \n",
            "\u001b[?25h  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 104 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 1.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we will use `nlptown/bert-base-multilingual-uncased-sentiment`. This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n",
        "\n",
        "The model can be used directly as a sentiment analysis model for product reviews in any of the six languages, or further finetuned on related sentiment analysis tasks. To keep the example small, we won't do any fine-tunning with our own data in this opportunity."
      ],
      "metadata": {
        "id": "JPpk7DelMLNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model with transformers"
      ],
      "metadata": {
        "id": "OjMhg9OAMZC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.auto import AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
        "\n",
        "model_uri = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_uri)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_uri)\n",
        "classifier = AutoModelForSequenceClassification.from_pretrained(model_uri, config=config)"
      ],
      "metadata": {
        "id": "9rZPxTeUMNpS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers library provides a convenient way to store all the artifacts of a given model, and that is using the functionsave_pretrained from the model."
      ],
      "metadata": {
        "id": "pIdqiKYeMtEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'rating_classifier'\n",
        "classifier.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "3H_7URIZMrwR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a single file called pytorch_model.bin which contains the weights of the model itself. However, remember that in order to run the model we also need it's corresponding tokenizer. The same save_pretrained method is available for the tokenizer, which will generate other set of files:"
      ],
      "metadata": {
        "id": "ZgYa-_aEMz4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l9J2S5gMxUz",
        "outputId": "8b9d0457-ab50-40e9-f400-da0f754d92ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('rating_classifier/tokenizer_config.json',\n",
              " 'rating_classifier/special_tokens_map.json',\n",
              " 'rating_classifier/vocab.txt',\n",
              " 'rating_classifier/added_tokens.json',\n",
              " 'rating_classifier/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the saved model using AllenNLP"
      ],
      "metadata": {
        "id": "m2HFRKFRM4GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'rating_classifier'"
      ],
      "metadata": {
        "id": "uTOh7YiANHhn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary"
      ],
      "metadata": {
        "id": "Sp9-h_tJN1dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.data.vocabulary import Vocabulary\n",
        "\n",
        "transformer_vocab = Vocabulary.from_pretrained_transformer(model_name)"
      ],
      "metadata": {
        "id": "PyRiNiAZNCsq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "The tokenization work is devided into 2 parts in AllenNLP, which allows to have a more modular approach:\n",
        "\n",
        "- Tokenizer: A Tokenizer splits strings of text into tokens. Typically, this either splits text into word tokens or character tokens. It's job is to split sequence of text into sequence of discreat words or tokens. It goes from text -> sequence of text.\n",
        "- Indexer: it's job is to take a sequence of tokens and translate them to word indexes in according to the dictionary. It goes from sequence of text -> sequences of indexes.\n"
      ],
      "metadata": {
        "id": "pPV85OLkNh8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import PretrainedTransformerTokenizer\n",
        "from allennlp.data.token_indexers.pretrained_transformer_indexer import PretrainedTransformerIndexer\n",
        "\n",
        "transformer_tokenizer = PretrainedTransformerTokenizer(model_name)\n",
        "token_indexer = PretrainedTransformerIndexer(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeDalMC6OA_k",
        "outputId": "6995ead8-a843-4fa6-9739-aecff52b2cbc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at rating_classifier were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedder\n",
        "\n",
        "The embedder job is to provide vectors for words. So it basically takes a word index in the dictionary and returns its vector representation."
      ],
      "metadata": {
        "id": "B-6SpGCeN76L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders.pretrained_transformer_embedder import PretrainedTransformerEmbedder"
      ],
      "metadata": {
        "id": "7fJI6xs-TuRp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedder = BasicTextFieldEmbedder({ \"tokens\": PretrainedTransformerEmbedder(model_name) })"
      ],
      "metadata": {
        "id": "o65Y2dgwWKQv"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note that AllenNLP support providing multiple embedders for different inputs. This is because it has this modular approach"
      ],
      "metadata": {
        "id": "aTrGfrfSeAIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `Seq2VecEncoder` is a Module that takes as input a sequence of vectors and returns a single vector. The input shape would be `(batch_size, sequence_length, input_dim)` and return a `(batch_size, output_dim)` tensor. In the BERT architecture, there is a pooling layer at the end of the BERT model. This returns an embedding for the [CLS] token, after passing it through a non-linear tanh activation; the non-linear layer is also part of the BERT model.\n",
        "\n",
        "We can create it in AllenNLP:"
      ],
      "metadata": {
        "id": "CeH5yBDdRj2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.modules.seq2vec_encoders.bert_pooler import BertPooler\n",
        "\n",
        "transformer_encoder = BertPooler(model_name)"
      ],
      "metadata": {
        "id": "-z7M1K5LRMNR"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "5pBqTastOMRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.models import BasicClassifier\n",
        "\n",
        "model = BasicClassifier(vocab=transformer_vocab, \n",
        "                        text_field_embedder=token_embedder, \n",
        "                        seq2vec_encoder=transformer_encoder, \n",
        "                        dropout=0.1, \n",
        "                        num_labels=5)"
      ],
      "metadata": {
        "id": "W4AVLaTiN-T6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model's weights"
      ],
      "metadata": {
        "id": "y4Ixrzl5OceX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model._classification_layer.weight = classifier.classifier.weight\n",
        "model._classification_layer.bias = classifier.classifier.bias"
      ],
      "metadata": {
        "id": "Nc8qrMwIObnW"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.eval()"
      ],
      "metadata": {
        "id": "kNfeM7ptOr26"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data readers\n",
        "\n",
        "AllenNLP uses the concept of DatasetReader which allows the creation of `Instance`'s which can provide the inputs in the format the model expects them. This abstraction allows the framwork to make any preprocessing needed before the data is actually sent to the model. "
      ],
      "metadata": {
        "id": "2v8m-77xOvTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "from allennlp.data import DatasetReader, Instance, Batch\n",
        "from allennlp.data.fields import Field, TextField, LabelField\n",
        "from allennlp.data.token_indexers import TokenIndexer\n",
        "from allennlp.data.tokenizers import Tokenizer\n",
        "\n",
        "class ClassificationTransformerReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer,\n",
        "        token_indexer: TokenIndexer,\n",
        "        max_tokens: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_indexers: Dict[str, TokenIndexer] = { \"tokens\": token_indexer }\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[: self.max_tokens]\n",
        "        \n",
        "        fields: Dict[str, Field] = { }\n",
        "        fields[\"tokens\"] = TextField(tokens, self.token_indexers)\n",
        "            \n",
        "        if label:\n",
        "            fields[\"label\"] = LabelField(label)\n",
        "            \n",
        "        return Instance(fields)"
      ],
      "metadata": {
        "id": "3MRob1AsOzmi"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2JSRogNMPg9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_reader = ClassificationTransformerReader(tokenizer=transformer_tokenizer, \n",
        "                                                 token_indexer=token_indexer, \n",
        "                                                 max_tokens=400)"
      ],
      "metadata": {
        "id": "CWYrgrtLPkss"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing if the reader works:"
      ],
      "metadata": {
        "id": "tve7x4u8Pr5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instance = dataset_reader.text_to_instance(\"this is a great read everyone should have\")"
      ],
      "metadata": {
        "id": "LjM3ZikBPrgI"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.nn import util\n",
        "\n",
        "dataset = Batch([instance])\n",
        "dataset.index_instances(transformer_vocab)\n",
        "model_input = util.move_to_device(dataset.as_tensor_dict(), model._get_prediction_device())"
      ],
      "metadata": {
        "id": "cDTaos_DXdaU"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.make_output_human_readable(model(**model_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CK0EdqLXh-3",
        "outputId": "04f9ece9-548d-49e7-c1e2-a31ad946659a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': ['4'],\n",
              " 'logits': tensor([[-2.3093, -2.4158, -0.5280,  1.7162,  2.7962]],\n",
              "        grad_fn=<AddmmBackward0>),\n",
              " 'probs': tensor([[0.0044, 0.0039, 0.0260, 0.2448, 0.7209]], grad_fn=<SoftmaxBackward0>),\n",
              " 'token_ids': tensor([[  101, 10372, 10127,   143, 11838, 18593, 36053, 14693, 10574,   102]]),\n",
              " 'tokens': [['[CLS]',\n",
              "   'this',\n",
              "   'is',\n",
              "   'a',\n",
              "   'great',\n",
              "   'read',\n",
              "   'everyone',\n",
              "   'should',\n",
              "   'have',\n",
              "   '[SEP]']]}"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the tokenizer and the model a single piece"
      ],
      "metadata": {
        "id": "ZgInyOHkP6ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.predictors import TextClassifierPredictor\n",
        "\n",
        "predictor = TextClassifierPredictor(model, dataset_reader)"
      ],
      "metadata": {
        "id": "VzY95panQB7E"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.predict(\"this is a great read everyone should have\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiPCscELXPra",
        "outputId": "195036cc-6b88-4e2e-c82e-e5c5184c3f8e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': '4',\n",
              " 'logits': [-2.309262275695801,\n",
              "  -2.415769338607788,\n",
              "  -0.5280130505561829,\n",
              "  1.7162026166915894,\n",
              "  2.7961645126342773],\n",
              " 'probs': [0.004371450748294592,\n",
              "  0.003929796162992716,\n",
              "  0.025954479351639748,\n",
              "  0.2448289394378662,\n",
              "  0.7209153175354004],\n",
              " 'token_ids': [101, 10372, 10127, 143, 11838, 18593, 36053, 14693, 10574, 102],\n",
              " 'tokens': ['[CLS]',\n",
              "  'this',\n",
              "  'is',\n",
              "  'a',\n",
              "  'great',\n",
              "  'read',\n",
              "  'everyone',\n",
              "  'should',\n",
              "  'have',\n",
              "  '[SEP]']}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bXQkKoaLXSaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}